This file is a merged representation of the entire codebase, combined into a single document by Repomix.

<file_summary>
This section contains a summary of this file.

<purpose>
This file contains a packed representation of the entire repository's contents.
It is designed to be easily consumable by AI systems for analysis, code review,
or other automated processes.
</purpose>

<file_format>
The content is organized as follows:
1. This summary section
2. Repository information
3. Directory structure
4. Repository files (if enabled)
5. Multiple file entries, each consisting of:
  - File path as an attribute
  - Full contents of the file
</file_format>

<usage_guidelines>
- This file should be treated as read-only. Any changes should be made to the
  original repository files, not this packed version.
- When processing this file, use the file path to distinguish
  between different files in the repository.
- Be aware that this file may contain sensitive information. Handle it with
  the same level of security as you would the original repository.
</usage_guidelines>

<notes>
- Some files may have been excluded based on .gitignore rules and Repomix's configuration
- Binary files are not included in this packed representation. Please refer to the Repository Structure section for a complete list of file paths, including binary files
- Files matching patterns in .gitignore are excluded
- Files matching default ignore patterns are excluded
- Files are sorted by Git change count (files with more changes are at the bottom)
</notes>

</file_summary>

<directory_structure>
backend/
  api/
    routes/
      documents.py
    chunking.py
    ingestion.py
    main.py
  db/
    migrations/
      0001_init.sql
    migrate.py
  repositories/
    documents.py
  services/
    embeddings.py
    qdrant.py
  config.py
infra/
  docker-compose.yml
.gitignore
.python-version
pyproject.toml
</directory_structure>

<files>
This section contains the contents of the repository's files.

<file path="backend/api/routes/documents.py">
import hashlib
from fastapi import APIRouter, BackgroundTasks, File, UploadFile, HTTPException
from pydantic import BaseModel
from ...repositories.documents import insert_document, find_document_by_hash
from ..ingestion import ingestion_pipeline
from ...services.qdrant import already_indexed


# 1. Define a Pydantic model for the response.
# This enables automatic validation and documentation.
class DocumentStatusResponse(BaseModel):
    id: int
    title: str
    file_hash: str
    status: str
    page_count: int | None = None


router = APIRouter(prefix="/documents", tags=["documents"])


def compute_sha256(data: bytes) -> str:
    return hashlib.sha256(data).hexdigest()


@router.post("/upload", response_model=DocumentStatusResponse)
async def upload_document(
    background_tasks: BackgroundTasks, file: UploadFile = File(...)
):
    if file.content_type not in ("application/pdf", "application/x-pdf"):
        raise HTTPException(status_code=400, detail="Only PDF files are supported.")
    data = await file.read()
    if not data:
        raise HTTPException(status_code=400, detail="Empty file.")
    file_hash = compute_sha256(data)

    existing = find_document_by_hash(file_hash)
    if existing:
        doc_id, title, status, page_count, _, _ = existing

        # 2. Add resilience: re-trigger ingestion for failed jobs or if missing from vector DB.
        if status == "failed":
            background_tasks.add_task(ingestion_pipeline, doc_id, data)
        elif status == "indexed" and not already_indexed(file_hash):
            background_tasks.add_task(ingestion_pipeline, doc_id, data)

        # 3. Return a dictionary that matches the Pydantic model.
        return {
            "id": doc_id,
            "title": title,
            "file_hash": file_hash,
            "status": status,
            "page_count": page_count,
        }

    title = file.filename or f"document-{file_hash[:8]}.pdf"
    doc_id = insert_document(title, file_hash)
    background_tasks.add_task(ingestion_pipeline, doc_id, data)

    return {
        "id": doc_id,
        "title": title,
        "file_hash": file_hash,
        "status": "queued",
        "page_count": None,
    }
</file>

<file path="backend/api/ingestion.py">
import logging
from typing import List, Any, cast
import fitz
from ..repositories.documents import update_status, get_document
from ..services.embeddings import embed_texts
from ..services.qdrant import (
    ensure_collection,
    already_indexed,
    build_points,
    upsert_points,
)
from .chunking import chunk_text


def parse_pdf_and_chunk(pdf_bytes: bytes) -> tuple[int, List[List[str]]]:
    doc = fitz.open(stream=pdf_bytes, filetype="pdf")
    page_chunks: List[List[str]] = []
    for page in doc:
        text = cast(Any, page).get_text("text")
        chunks = chunk_text(text)
        page_chunks.append(chunks)
    return doc.page_count, page_chunks


def ingestion_pipeline(doc_id: int, pdf_bytes: bytes) -> None:
    try:
        update_status(doc_id, "parsing")
        page_count, page_chunks = parse_pdf_and_chunk(pdf_bytes)

        update_status(doc_id, "embedding", page_count=page_count)

        doc_row = get_document(doc_id)
        if not doc_row:
            # This would be a serious internal error
            logging.error(f"Document {doc_id} disappeared during ingestion.")
            raise RuntimeError(f"Document {doc_id} not found")
        _, doc_title, file_hash, *_ = doc_row

        ensure_collection()

        if already_indexed(file_hash):
            logging.warning(
                f"Skipping indexing for doc_id {doc_id} as file_hash {file_hash} is already indexed in Qdrant."
            )
            update_status(doc_id, "indexed", page_count=page_count)
            return

        texts_with_meta = []
        texts = []
        for page_idx, chunks in enumerate(page_chunks, start=1):
            for chunk_idx, text in enumerate(chunks):
                if text.strip():
                    texts_with_meta.append((text, page_idx, chunk_idx))
                    texts.append(text)
        if texts:
            vectors = embed_texts(texts)
            points = build_points(
                doc_id, doc_title, file_hash, texts_with_meta, vectors
            )
            upsert_points(points)

        update_status(doc_id, "indexed", page_count=page_count)
    except Exception as e:
        logging.error(
            f"Ingestion pipeline failed for doc_id {doc_id}: {e}", exc_info=True
        )
        update_status(doc_id, "failed")
</file>

<file path="backend/repositories/documents.py">
import psycopg2
from typing import Optional, Tuple
from .. import config


def db_conn():
    return psycopg2.connect(
        f"host={config.POSTGRES_HOST} port={config.POSTGRES_PORT} dbname={config.POSTGRES_DB} user={config.POSTGRES_USER} password={config.POSTGRES_PASSWORD}"
    )


def insert_document(title: str, file_hash: str) -> int:
    with db_conn() as conn:
        with conn.cursor() as cur:
            cur.execute(
                """
				INSERT INTO documents (title, file_hash, status)
				VALUES (%s, %s, 'queued')
				RETURNING id;
				""",
                (title, file_hash),
            )
            row = cur.fetchone()
            if row is None:
                raise RuntimeError("Insert did not return id")
            doc_id = row[0]
            conn.commit()
            return doc_id


def find_document_by_hash(file_hash: str) -> Optional[Tuple]:
    with db_conn() as conn:
        with conn.cursor() as cur:
            cur.execute(
                """
				SELECT id, title, status, page_count, created_at, updated_at
				FROM documents
				WHERE file_hash = %s
				LIMIT 1;
				""",
                (file_hash,),
            )
            return cur.fetchone()


def get_document(doc_id: int) -> Optional[Tuple]:
    with db_conn() as conn:
        with conn.cursor() as cur:
            cur.execute(
                """
				SELECT id, title, file_hash, status, page_count, created_at, updated_at
				FROM documents
				WHERE id = %s
				LIMIT 1;
				""",
                (doc_id,),
            )
            return cur.fetchone()


def update_status(doc_id: int, status: str, page_count: int | None = None) -> None:
    with db_conn() as conn:
        with conn.cursor() as cur:
            if page_count is None:
                cur.execute(
                    "UPDATE documents SET status = %s, updated_at = now() WHERE id = %s;",
                    (status, doc_id),
                )
            else:
                cur.execute(
                    "UPDATE documents SET status = %s, page_count = %s, updated_at = now() WHERE id = %s;",
                    (status, page_count, doc_id),
                )
            conn.commit()
</file>

<file path="backend/services/embeddings.py">
from typing import List, Optional
from sentence_transformers import SentenceTransformer
from .. import config
import numpy as np


_EMBEDDER: Optional[SentenceTransformer] = None


def get_model() -> SentenceTransformer:
    global _EMBEDDER
    if _EMBEDDER is None:
        _EMBEDDER = SentenceTransformer(config.EMBEDDING_MODEL_NAME)
    return _EMBEDDER


def embedding_dimension() -> int:
    m = get_model()
    method = getattr(m, "get_sentence_embedding_dimension", None)
    if callable(method):
        d = method()
        if isinstance(d, int):
            return d
    # Fallback: infer from a tiny forward pass
    arr = m.encode([""], convert_to_numpy=True)
    try:
        return int(arr.shape[1])  # type: ignore[attr-defined]
    except Exception:
        return int(len(arr[0]))


def embedding_model_name() -> str:
    m = get_model()
    method = getattr(m, "get_sentence_embedding_model_name", None)
    if callable(method):
        v = method()
        if isinstance(v, str) and v:
            return v
    return str(config.EMBEDDING_MODEL_NAME)


def embed_texts(texts: List[str]) -> List[List[float]]:
    model = get_model()
    arr = model.encode(
        texts,
        batch_size=config.EMBED_BATCH_SIZE,
        convert_to_numpy=True,
        show_progress_bar=False,
        normalize_embeddings=False,
    )
    if isinstance(arr, np.ndarray):
        arr = arr.astype("float32")
    return [row.tolist() for row in arr]
</file>

<file path="backend/services/qdrant.py">
import re
import logging
from typing import List, Tuple
from datetime import datetime
from qdrant_client import QdrantClient
from qdrant_client.http.models import (
    Distance,
    VectorParams,
    PointStruct,
    Filter,
    FieldCondition,
    MatchValue,
)
from .. import config
from .embeddings import embedding_dimension, embedding_model_name

_CLIENT: QdrantClient | None = None


def client() -> QdrantClient:
    global _CLIENT
    if _CLIENT is None:
        if config.QDRANT_URL:
            _CLIENT = QdrantClient(url=config.QDRANT_URL, api_key=config.QDRANT_API_KEY)
        else:
            _CLIENT = QdrantClient(
                host=config.QDRANT_HOST,
                port=config.QDRANT_PORT,
                api_key=config.QDRANT_API_KEY,
            )
    return _CLIENT


def collection_name() -> str:
    model = embedding_model_name()
    safe = re.sub(r"[^A-Za-z0-9]+", "_", model).strip("_")
    return f"pdf_chunks__{safe}"


def ensure_collection() -> str:
    name = collection_name()
    vec_size = embedding_dimension()
    try:
        client().get_collection(name)
    except Exception:
        client().create_collection(
            collection_name=name,
            vectors_config=VectorParams(size=vec_size, distance=Distance.COSINE),
        )
    return name


def already_indexed(file_hash: str) -> bool:
    try:
        res = client().count(
            collection_name=collection_name(),
            count_filter=Filter(
                must=[
                    FieldCondition(key="file_hash", match=MatchValue(value=file_hash))
                ]
            ),
            exact=True,
        )
        return (res.count or 0) > 0
    except Exception as e:
        logging.error(
            f"Failed to check Qdrant index for hash {file_hash}: {e}", exc_info=True
        )
        return False


def upsert_points(points: List[PointStruct]) -> None:
    name = ensure_collection()
    for i in range(0, len(points), config.QDRANT_UPSERT_BATCH):
        client().upsert(
            collection_name=name, points=points[i : i + config.QDRANT_UPSERT_BATCH]
        )


def build_points(
    doc_id: int,
    doc_title: str,
    file_hash: str,
    texts_with_meta: List[Tuple[str, int, int]],
    vectors: List[List[float]],
) -> List[PointStruct]:
    now = datetime.utcnow().isoformat() + "Z"
    points: List[PointStruct] = []
    for (text, page, chunk_idx), vec in zip(texts_with_meta, vectors):
        points.append(
            PointStruct(
                id=f"{file_hash}:{page}:{chunk_idx}",
                vector=vec,
                payload={
                    "doc_id": doc_id,
                    "doc_title": doc_title,
                    "page": page,
                    "chunk_id": chunk_idx,
                    "chunk_text": text,
                    "file_hash": file_hash,
                    "created_at": now,
                    "updated_at": now,
                },
            )
        )
    return points
</file>

<file path="backend/config.py">
import os

POSTGRES_HOST = os.getenv("POSTGRES_HOST", "localhost")
POSTGRES_PORT = int(os.getenv("POSTGRES_PORT", "5432"))
POSTGRES_DB = os.getenv("POSTGRES_DB", "ragdb")
POSTGRES_USER = os.getenv("POSTGRES_USER", "rag")
POSTGRES_PASSWORD = os.getenv("POSTGRES_PASSWORD", "ragpass")

EMBEDDING_MODEL_NAME = os.getenv(
    "EMBEDDING_MODEL_NAME", "sentence-transformers/all-MiniLM-L6-v2"
)
EMBED_BATCH_SIZE = int(os.getenv("EMBED_BATCH_SIZE", "64"))

QDRANT_URL = os.getenv("QDRANT_URL")
QDRANT_API_KEY = os.getenv("QDRANT_API_KEY")
QDRANT_HOST = os.getenv("QDRANT_HOST", "localhost")
QDRANT_PORT = int(os.getenv("QDRANT_PORT", "6333"))
QDRANT_UPSERT_BATCH = int(os.getenv("QDRANT_UPSERT_BATCH", "128"))
</file>

<file path="backend/api/chunking.py">
import os
from typing import List


def chunk_text(
    text: str, chunk_size: int | None = None, overlap: int | None = None
) -> List[str]:
    # Defaults via env or sane fallbacks
    if chunk_size is None:
        chunk_size = int(os.getenv("CHUNK_SIZE", "1200"))
    if overlap is None:
        overlap = int(os.getenv("CHUNK_OVERLAP", "200"))
    chunks: List[str] = []
    i = 0
    n = len(text)
    if n == 0:
        return chunks
    while i < n:
        j = min(i + chunk_size, n)
        chunks.append(text[i:j])
        if j == n:
            break
        i = j - overlap if j - overlap > i else j
    return chunks
</file>

<file path="backend/api/main.py">
from fastapi import FastAPI
from .routes.documents import router as documents_router

app = FastAPI(title="AlgorithmX RAG Backend", version="0.1.0")

app.include_router(documents_router)
</file>

<file path="backend/db/migrations/0001_init.sql">
-- documents table for tracking PDF ingestion lifecycle
CREATE TABLE IF NOT EXISTS documents (
	id BIGSERIAL PRIMARY KEY,
	title TEXT NOT NULL,
	file_hash CHAR(64) NOT NULL,
	status VARCHAR(32) NOT NULL DEFAULT 'queued',
	page_count INTEGER,
	created_at TIMESTAMPTZ NOT NULL DEFAULT now(),
	updated_at TIMESTAMPTZ NOT NULL DEFAULT now()
);

-- indexes
CREATE UNIQUE INDEX IF NOT EXISTS idx_documents_file_hash ON documents(file_hash);
CREATE INDEX IF NOT EXISTS idx_documents_status ON documents(status);
</file>

<file path="backend/db/migrate.py">
import os
import sys
from pathlib import Path

import psycopg2


def conninfo_from_env() -> str:
    HOST = os.getenv("POSTGRES_HOST", "localhost")
    PORT = os.getenv("POSTGRES_PORT", "5432")
    DB = os.getenv("POSTGRES_DB", "ragdb")
    USER = os.getenv("POSTGRES_USER", "rag")
    PASSWORD = os.getenv("POSTGRES_PASSWORD", "ragpass")
    return f"host={HOST} port={PORT} dbname={DB} user={USER} password={PASSWORD}"


def ensure_schema_migrations(cur) -> None:
    cur.execute(
        """
		CREATE TABLE IF NOT EXISTS schema_migrations (
			filename TEXT PRIMARY KEY,
			applied_at TIMESTAMPTZ NOT NULL DEFAULT now()
		);
		"""
    )


def migration_applied(cur, filename: str) -> bool:
    cur.execute("SELECT 1 FROM schema_migrations WHERE filename = %s;", (filename,))
    return cur.fetchone() is not None


def apply_sql(cur, sql_text: str) -> None:
    # naive splitter on ';' to avoid multi-statement issues; fine for our simple SQL
    statements = [s.strip() for s in sql_text.split(";") if s.strip()]
    for stmt in statements:
        cur.execute(stmt + ";")


def main() -> int:
    migrations_dir = Path(__file__).parent / "migrations"
    if not migrations_dir.exists():
        print(f"Missing migrations directory: {migrations_dir}", file=sys.stderr)
        return 1

    conninfo = conninfo_from_env()
    with psycopg2.connect(conninfo) as conn:
        with conn.cursor() as cur:
            cur.execute("SET SESSION CHARACTERISTICS AS TRANSACTION READ WRITE;")
            ensure_schema_migrations(cur)
            applied = 0
            for path in sorted(migrations_dir.glob("*.sql")):
                name = path.name
                if migration_applied(cur, name):
                    continue
                sql_text = path.read_text(encoding="utf-8")
                apply_sql(cur, sql_text)
                cur.execute(
                    "INSERT INTO schema_migrations (filename) VALUES (%s);",
                    (name,),
                )
                applied += 1
            conn.commit()
            print(f"Applied {applied} migration(s).")
    return 0


if __name__ == "__main__":
    sys.exit(main())
</file>

<file path="infra/docker-compose.yml">
services:
  postgres:
    image: postgres:17
    container_name: algx_postgres
    restart: unless-stopped
    environment:
      POSTGRES_USER: rag
      POSTGRES_PASSWORD: ragpass
      POSTGRES_DB: ragdb
    ports:
      - "5432:5432"
    volumes:
      - postgres_data:/var/lib/postgresql/data
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U rag || exit 1"]
      interval: 10s
      timeout: 5s
      retries: 5
      start_period: 10s

  qdrant:
    image: qdrant/qdrant:latest
    container_name: algx_qdrant
    restart: unless-stopped
    ports:
      - "6333:6333"
      - "6334:6334"
    volumes:
      - qdrant_data:/qdrant/storage
    healthcheck:
      test: ["CMD-SHELL", "wget -q --spider http://localhost:6333/readyz || exit 1"]
      interval: 10s
      timeout: 5s
      retries: 10
      start_period: 10s

volumes:
  postgres_data:
  qdrant_data:
</file>

<file path=".gitignore">
# Python-generated files
__pycache__/
*.py[oc]
build/
dist/
wheels/
*.egg-info

# Virtual environments
.venv

.env
</file>

<file path=".python-version">
3.12
</file>

<file path="pyproject.toml">
[project]
name = "algorithmx-assignment"
version = "0.1.0"
description = "Add your description here"
readme = "README.md"
requires-python = ">=3.12"
dependencies = [
    "fastapi[standard]>=0.118.0",
    "numpy>=2.3.3",
    "psycopg2-binary>=2.9.10",
    "pydantic>=2.11.9",
    "pymupdf>=1.26.4",
    "python-multipart>=0.0.20",
    "qdrant-client>=1.15.1",
    "sentence-transformers>=5.1.1",
]
</file>

</files>
